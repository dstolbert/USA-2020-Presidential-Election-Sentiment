{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Twitter keys\n",
    "\n",
    "from keys import consumer_key, consumer_secret, access_token, access_token_secret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import tweepy\n",
    "from tweepy import OAuthHandler \n",
    "from textblob import TextBlob\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get state geolocations\n",
    "\n",
    "coords = pd.read_csv('state_coords.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A quick word on the notebook\n",
    "\n",
    "The purpose of this notebook is to extract the relevant Twitter data and to get baseline sentiment classifications using the TextBlob module. The code I am going to use for the TwitterClient class is not my own and is taken from https://www.geeksforgeeks.org/twitter-sentiment-analysis-using-python/\n",
    "\n",
    "### Creating the TwitterClient class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwitterClient(object): \n",
    "    ''' \n",
    "    Generic Twitter Class for sentiment analysis. \n",
    "    '''\n",
    "    def __init__(self): \n",
    "        ''' \n",
    "        Class constructor or initialization method. \n",
    "        '''  \n",
    "        # attempt authentication \n",
    "        try: \n",
    "            # create OAuthHandler object \n",
    "            self.auth = OAuthHandler(consumer_key, consumer_secret) \n",
    "            # set access token and secret \n",
    "            self.auth.set_access_token(access_token, access_token_secret) \n",
    "            # create tweepy API object to fetch tweets \n",
    "            self.api = tweepy.API(self.auth, wait_on_rate_limit=True, wait_on_rate_limit_notify=True) \n",
    "        except: \n",
    "            print(\"Error: Authentication Failed\") \n",
    "  \n",
    "    def clean_tweet(self, tweet): \n",
    "        ''' \n",
    "        Utility function to clean tweet text by removing links, special characters \n",
    "        using simple regex statements. \n",
    "        '''\n",
    "        return ' '.join(re.sub(\"(@[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)\", \" \", tweet).split()) \n",
    "  \n",
    "    def get_tweet_sentiment(self, tweet): \n",
    "        ''' \n",
    "        Utility function to classify sentiment of passed tweet \n",
    "        using textblob's sentiment method \n",
    "        '''\n",
    "        # create TextBlob object of passed tweet text \n",
    "        analysis = TextBlob(self.clean_tweet(tweet)) \n",
    "        # set sentiment \n",
    "        if analysis.sentiment.polarity > 0: \n",
    "            return 'positive'\n",
    "        elif analysis.sentiment.polarity == 0: \n",
    "            return 'neutral'\n",
    "        else: \n",
    "            return 'negative'\n",
    "  \n",
    "    def get_tweets(self, query, geocode,count=10): \n",
    "        ''' \n",
    "        Main function to fetch tweets and parse them. \n",
    "        '''\n",
    "        # empty list to store parsed tweets \n",
    "        tweets = [] \n",
    "  \n",
    "        try: \n",
    "            # call twitter api to fetch tweets \n",
    "            fetched_tweets = self.api.search(q = query, count = count, geocode=geocode) \n",
    "  \n",
    "            # parsing tweets one by one \n",
    "            for tweet in fetched_tweets: \n",
    "                # empty dictionary to store required params of a tweet \n",
    "                parsed_tweet = {} \n",
    "                \n",
    "                # saving candidate name\n",
    "                parsed_tweet['candidate'] = query\n",
    "                # saving text of tweet \n",
    "                parsed_tweet['text'] = tweet.text \n",
    "                # saving sentiment of tweet \n",
    "                parsed_tweet['sentiment'] = self.get_tweet_sentiment(tweet.text) \n",
    "  \n",
    "                # appending parsed tweet to tweets list \n",
    "                if tweet.retweet_count > 0: \n",
    "                    # if tweet has retweets, ensure that it is appended only once \n",
    "                    if parsed_tweet not in tweets: \n",
    "                        tweets.append(parsed_tweet) \n",
    "                else: \n",
    "                    tweets.append(parsed_tweet) \n",
    "  \n",
    "            # return parsed tweets \n",
    "            return tweets \n",
    "  \n",
    "        except tweepy.TweepError as e: \n",
    "            # print error (if any) \n",
    "            print(\"Error : \" + str(e)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The method\n",
    "\n",
    "Now we will instantiate the TwitterClient class, generate a query, hit the Twitter api, save tweets in a dataframe, calculate summary stats to save in a seperate dataframe.\n",
    "\n",
    "* Loop through 51 states (inluding DC) for each candidate using the approximate coordinate centers of each state for a 100mile radius\n",
    "\n",
    "### The problems with the method\n",
    "\n",
    "I will be searching for the current top 3 democratic candidates as well as the current US president. Additionally I will use geolocation to get tweets from each state. Because I am using a standard account, I am limited to 250 queries per month. Therefore I will need to restrict my queries to 1 per state to be safe. While this will negatively effect the quality of our samples, we should still be able to get some interesting analyses. States like California and Texas will be negatively effected by this method due to the large land area in those states. If everything goes smoothly I may increase my search queries in those states to address this issue. Additionally, there is the chance that the geocode reaches outside of a state (i.e. Rhode Island)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "api = TwitterClient()\n",
    "candidates = ['Bernie Sanders', 'Elizabeth Warren', 'Joe Biden', 'Donald Trump']\n",
    "\n",
    "df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Rate limit reached. Sleeping for: 191\n",
      "Rate limit reached. Sleeping for: 782\n"
     ]
    }
   ],
   "source": [
    "tweets = []\n",
    "\n",
    "# Loop through cadidates and geolocations \n",
    "\n",
    "for candidate in candidates:\n",
    "    for i in range(coords.shape[0]):\n",
    "        lat = coords.loc[i, 'Lat']\n",
    "        lng = coords.loc[i,'Lng']\n",
    "        geocode = f'{lat},{lng},100mi'\n",
    "        \n",
    "        tweets = tweets + api.get_tweets(query = candidate, \n",
    "                                         count=100, \n",
    "                                         geocode=geocode)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save tweets in a df\n",
    "\n",
    "for i in range(len(tweets)):\n",
    "    df.loc[i, 'candidate'] = tweets[i]['candidate']\n",
    "    df.loc[i, 'text'] = tweets[i]['text']\n",
    "    df.loc[i, 'sentiment'] = tweets[i]['sentiment']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how many tweets I have per candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "candidate\n",
       "Bernie Sanders      2401\n",
       "Donald Trump        2796\n",
       "Elizabeth Warren    1919\n",
       "Joe Biden           2553\n",
       "dtype: int64"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby(['candidate']).size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save df for future use\n",
    "\n",
    "df.to_csv('pres_tweets.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
